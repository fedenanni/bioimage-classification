{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce8a4f4",
   "metadata": {},
   "source": [
    "# Phenotyping of Fly Wings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a948d",
   "metadata": {},
   "source": [
    "## 1. Open the Dataset and Browse the Available Features\n",
    "\n",
    "In the folder \"data\" you will find the dataset used in the paper [Assessing the potential of vision language models for automated phenotyping of Drosophila melanogaster](https://www.biorxiv.org/content/biorxiv/early/2024/05/27/2024.05.27.594652.full.pdf). The dataset is composed of a XLSX file and an additional \"data\" folder containing the images associated with each fly wing. The first task is to:\n",
    "1. open the dataset and explore it\n",
    "2. examine whether any information is missing\n",
    "3. using the paper, study how the dataset was created\n",
    "4. find a way to open the dataset using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399d86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: feel free to use an AI tool for learning how to open the dataset, but you should be able to explain what the tool does"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86619a72",
   "metadata": {},
   "source": [
    "# 2. Perform a Classification\n",
    "\n",
    "By examining the dataset you can select a feature you want to predict based on the image. It could be, as in the paper linked above, the aberrant vs wildtype phenotye or it could be predicting generally the type of phenotype or it could be something else of your own choice. Compare the performance of two of the classified discussed in class. Be careful, in this setting you don't have many training data - are there other evaluation practices that can help you assessing performance apart from train/test split? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: you need two things: 1) to be able to open in Python the images and 2) to select a label from the spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ae5af",
   "metadata": {},
   "source": [
    "# 3. Examine the Errors the Tools Make\n",
    "\n",
    "Compare the errors the classifiers make (which images are mis-labelled?). Do you notice any specific type of pattern? How good are performance overall? Would you use a tool like this as part of an experimental pipeline? What would be the trade-offs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca07e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: you need to associate each classified image with the predicted label. Then you can manually examine a few of the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ddce6",
   "metadata": {},
   "source": [
    "# 4. Optional: LLMs for Image Classification\n",
    "\n",
    "In the paper linked above, we used a LLM for image classification. You can try the same and then discuss what do you think are the major differences compared to training a machine learning classifier. Which benefits do you see? What are the drawbacks?\n",
    "\n",
    "Feel free to use any LLM you want. You don't need to redo the entire classification, a specific experiment to compare performance will be good enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioimage-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
